{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from functools import reduce\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from peft.tuners import lora\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.data.data_collator import DataCollator\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n",
    "from transformers.trainer import (EvalPrediction, PreTrainedModel,\n",
    "                                  PreTrainedTokenizerBase, TrainerCallback)\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.utils import is_sagemaker_mp_enabled, logging\n",
    "\n",
    "if is_sagemaker_mp_enabled():\n",
    "    import smdistributed.modelparallel.torch as smp\n",
    "\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LoraPlusTrainingArguments(TrainingArguments):\n",
    "    do_train: bool = field(default=True, metadata={\"help\": \"Whether to run training.\"})\n",
    "    do_eval: bool = field(\n",
    "        default=True, metadata={\"help\": \"Whether to run eval on the dev set.\"}\n",
    "    )\n",
    "    keep_checkpoints: str = field(\n",
    "        default=\"all\",\n",
    "        metadata={\"help\": \"keep all, eval, or none checkpoints after end of training\"},\n",
    "    )\n",
    "    lora_rank: int = field(default=8, metadata={\"help\": \"LoRA rank r\"})\n",
    "    lora_alpha: float = field(default=16, metadata={\"help\": \"LoRA alpha parameter\"})\n",
    "    lora_dropout: float = field(\n",
    "        default=0.1, metadata={\"help\": \"dropout rate for LoRA modules\"}\n",
    "    )\n",
    "    target_modules: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"which modules to add LoRA layer to\"}\n",
    "    )\n",
    "    use_lora: bool = field(\n",
    "        default=True, metadata={\"help\": \"whether to finetune using LoRA\"}\n",
    "    )\n",
    "    lora_use_original_init: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"whether to use the original LoRA initialization\"},\n",
    "    )\n",
    "    bf16: bool = field(default=False, metadata={\"help\": \"use bfloat16\"})\n",
    "    fp16: bool = field(default=False, metadata={\"help\": \"use bfloat16\"})\n",
    "    gradient_checkpointing: bool = field(\n",
    "        default=False, metadata={\"help\": \"use gradient checkpointing\"}\n",
    "    )\n",
    "    loraplus_lr_ratio: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"loraplus learning rate ratio lr_B / lr_A.\"}\n",
    "    )\n",
    "    loraplus_lr_embedding: Optional[float] = field(\n",
    "        default=1e-6, metadata={\"help\": \"loraplus learning rate for lora embedding layers.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "def get_module(name, opt_model):\n",
    "    parent_idx = 2 if \"lora\" in name else 1\n",
    "    module_names = name.split(sep=\".\")[:-parent_idx]\n",
    "    module = reduce(getattr, module_names, opt_model)\n",
    "    return module\n",
    "\n",
    "def create_loraplus_optimizer(\n",
    "    opt_model,\n",
    "    optimizer_cls,\n",
    "    optimizer_kwargs,\n",
    "    loraplus_lr_ratio,\n",
    "    loraplus_lr_embedding=None,\n",
    "):\n",
    "\n",
    "    assert loraplus_lr_ratio is not None, \"loraplus_lr_ratio must be provided.\"\n",
    "\n",
    "    if loraplus_lr_embedding is None:\n",
    "        loraplus_lr_embedding = 1e-6\n",
    "\n",
    "    decay_parameters = get_parameter_names(opt_model, ALL_LAYERNORM_LAYERS)\n",
    "    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "    param_groups = {\n",
    "        \"groupA\": {},\n",
    "        \"groupB\": {},\n",
    "        \"groupB_no_decay\": {},\n",
    "        \"embedding\": {},\n",
    "    }\n",
    "\n",
    "    for name, param in opt_model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "\n",
    "        module = get_module(name, opt_model)\n",
    "        if isinstance(module, lora.Embedding):\n",
    "            param_groups[\"embedding\"][name] = param\n",
    "        elif \"lora_B\" in name or param.ndim == 1:\n",
    "            if name in decay_parameters:\n",
    "                param_groups[\"groupB\"][name] = param\n",
    "            else:\n",
    "                param_groups[\"groupB_no_decay\"][name] = param\n",
    "        else:\n",
    "            param_groups[\"groupA\"][name] = param\n",
    "\n",
    "    assigned_param_groups = \"\"\n",
    "    for group in param_groups:\n",
    "        assigned_param_groups += f\"{group}\\n {list(param_groups[group].keys())}\\n\\n\"\n",
    "    logger.debug(assigned_param_groups)\n",
    "\n",
    "    lr = optimizer_kwargs[\"lr\"]\n",
    "    weight_decay = optimizer_kwargs.get(\"weight_decay\", 0.0)\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": list(param_groups[\"groupA\"].values()),\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"lr\": lr,\n",
    "        },\n",
    "        {\n",
    "            \"params\": list(param_groups[\"embedding\"].values()),\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"lr\": loraplus_lr_embedding,\n",
    "        },\n",
    "        {\n",
    "            \"params\": list(param_groups[\"groupB\"].values()),\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"lr\": lr * loraplus_lr_ratio,\n",
    "        },\n",
    "        {\n",
    "            \"params\": list(param_groups[\"groupB_no_decay\"].values()),\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"lr\": lr * loraplus_lr_ratio,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n",
    "    if optimizer_cls.__name__ == \"Adam8bit\":\n",
    "        import bitsandbytes\n",
    "\n",
    "        manager = bitsandbytes.optim.GlobalOptimManager.get_instance()\n",
    "\n",
    "        skipped = 0\n",
    "        for module in opt_model.modules():\n",
    "            if isinstance(module, nn.Embedding):\n",
    "                skipped += sum(\n",
    "                    {p.data_ptr(): p.numel() for p in module.parameters()}.values()\n",
    "                )\n",
    "                logger.info(f\"skipped {module}: {skipped/2**20}M params\")\n",
    "                manager.register_module_override(module, \"weight\", {\"optim_bits\": 32})\n",
    "                logger.debug(f\"bitsandbytes: will optimize {module} in fp32\")\n",
    "        logger.info(f\"skipped: {skipped/2**20}M params\")\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "class LoraPlusTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Union[PreTrainedModel, nn.Module] = None,\n",
    "        args: LoraPlusTrainingArguments = None,\n",
    "        data_collator: Optional[DataCollator] = None,\n",
    "        train_dataset: Optional[Dataset] = None,\n",
    "        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n",
    "        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n",
    "        model_init: Optional[Callable[[], PreTrainedModel]] = None,\n",
    "        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n",
    "        callbacks: Optional[List[TrainerCallback]] = None,\n",
    "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),\n",
    "        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n",
    "    ):\n",
    "        assert isinstance(args, LoraPlusTrainingArguments), \"args must be of type LoraPlusTrainingArguments\"\n",
    "        super().__init__(\n",
    "            model,\n",
    "            args,\n",
    "            data_collator,\n",
    "            train_dataset,\n",
    "            eval_dataset,\n",
    "            tokenizer,\n",
    "            model_init,\n",
    "            compute_metrics,\n",
    "            callbacks,\n",
    "            optimizers,\n",
    "            preprocess_logits_for_metrics,\n",
    "        )\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        if self.args.loraplus_lr_ratio is None:\n",
    "            return super().create_optimizer()\n",
    "\n",
    "        opt_model = self.model_wrapped if is_sagemaker_mp_enabled() else self.model\n",
    "        if self.optimizer is None:\n",
    "            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(\n",
    "                self.args\n",
    "            )\n",
    "\n",
    "            loraplus_lr_ratio = getattr(self.args, 'loraplus_lr_ratio', None)\n",
    "            loraplus_lr_embedding = getattr(self.args, 'loraplus_lr_embedding', None)\n",
    "            self.optimizer = create_loraplus_optimizer(\n",
    "                opt_model,\n",
    "                optimizer_cls,\n",
    "                optimizer_kwargs,\n",
    "                loraplus_lr_ratio,\n",
    "                loraplus_lr_embedding,\n",
    "            )\n",
    "\n",
    "        if is_sagemaker_mp_enabled():\n",
    "            self.optimizer = smp.DistributedOptimizer(self.optimizer)\n",
    "\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 502/502 [00:00<00:00, 1.19MB/s]\n",
      "model.safetensors: 100%|██████████| 346M/346M [00:03<00:00, 114MB/s]  \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    label2id={},\n",
    "    id2label={},\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")\n",
    "# print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.vit.modeling_vit.ViTForImageClassification"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
